{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom template to guide llm model\n",
    "custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from pdf\n",
    "def get_pdf_text(docs):\n",
    "    text=\"\"\n",
    "    for pdf in docs:\n",
    "        pdf_reader=PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text+=page.extract_text()\n",
    "    return text\n",
    "\n",
    "# converting text to chunks\n",
    "def get_chunks(raw_text):\n",
    "    text_splitter=CharacterTextSplitter(separator=\"\\n\",\n",
    "                                        chunk_size=200,\n",
    "                                        chunk_overlap=50,\n",
    "                                        length_function=len)   \n",
    "    chunks=text_splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all-MiniLm embeddings model and faiss to get vectorstore\n",
    "def get_vectorstore(chunks):\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                     model_kwargs={'device':'cpu'})\n",
    "    vectorstore=faiss.FAISS.from_texts(texts=chunks,embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating conversation chain  \n",
    "\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def get_conversationchain(vectorstore):\n",
    "    llm = VLLM(\n",
    "        model=\"baichuan-inc/Baichuan2-13B-Chat\",\n",
    "        trust_remote_code=True,  # mandatory for hf models\n",
    "        max_new_tokens=512,\n",
    "        top_k=20,\n",
    "        top_p=0.8,\n",
    "        temperature=0.8,\n",
    "        dtype=\"float16\",\n",
    "        tensor_parallel_size=4\n",
    "        )\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', \n",
    "                                      return_messages=True,\n",
    "                                      output_key='answer') # using conversation buffer memory to hold past information\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                                llm=llm,\n",
    "                                retriever=vectorstore.as_retriever(),\n",
    "                                condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "                                memory=memory)\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"document_sample/China_Lake_Energetics_Brochures_Proposal_4.pdf\", \"document_sample/Design-West_FAMS-S_Proposal_signed.pdf\"]\n",
    "raw_text=get_pdf_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 209, which is longer than the specified 200\n",
      "Created a chunk of size 322, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 256, which is longer than the specified 200\n",
      "Created a chunk of size 242, which is longer than the specified 200\n",
      "Created a chunk of size 273, which is longer than the specified 200\n",
      "Created a chunk of size 252, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 362, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 266, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_chunks=get_chunks(raw_text)\n",
    "vectorstore=get_vectorstore(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_question(conversation_chain, question):\n",
    "    response = conversation_chain({'question': question})\n",
    "    chat_history = response[\"chat_history\"]\n",
    "    for i, msg in enumerate(chat_history):\n",
    "        if i % 2 == 0:\n",
    "            print(f\"User: {msg.content}\")\n",
    "        else:\n",
    "            print(f\"Bot: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-01 20:50:34 config.py:1222] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-08-01 20:50:36,394\tINFO worker.py:1770 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 20:50:37 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 08-01 20:50:37 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='baichuan-inc/Baichuan2-13B-Chat', speculative_config=None, tokenizer='baichuan-inc/Baichuan2-13B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=baichuan-inc/Baichuan2-13B-Chat)\n",
      "WARNING 08-01 20:50:38 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n"
     ]
    },
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0004, num_gpus=\u0004\n\nCUDA call was originally invoked at:\n\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_27318/2183081615.py\", line 2, in <module>\n    vectorstore=get_vectorstore(text_chunks)\n  File \"/tmp/ipykernel_27318/1057278163.py\", line 3, in get_vectorstore\n    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\n    return wrapped(self, *args, **kwargs)\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 71, in __init__\n    import sentence_transformers\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/__init__.py\", line 7, in <module>\n    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py\", line 1, in <module>\n    from .CrossEncoder import CrossEncoder\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:306\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:174\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 174\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:448\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0004, num_gpus=\u0004",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m \u001b[43mget_conversationchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m handle_question(conversation_chain, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere will ARA travel to for CBOA 2024?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mget_conversationchain\u001b[0;34m(vectorstore)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_conversationchain\u001b[39m(vectorstore):\n\u001b[0;32m----> 8\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaichuan-inc/Baichuan2-13B-Chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# mandatory for hf models\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     19\u001b[0m                                       return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m                                       output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# using conversation buffer memory to hold past information\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     conversation_chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[1;32m     22\u001b[0m                                 llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     23\u001b[0m                                 retriever\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[1;32m     24\u001b[0m                                 condense_question_prompt\u001b[38;5;241m=\u001b[39mCUSTOM_QUESTION_PROMPT,\n\u001b[1;32m     25\u001b[0m                                 memory\u001b[38;5;241m=\u001b[39mmemory)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/pydantic/v1/main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_community/llms/vllm.py:88\u001b[0m, in \u001b[0;36mVLLM.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import vllm python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install vllm`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mVLLModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensor_parallel_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdownload_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvllm_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:144\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    124\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    125\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    126\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:363\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    360\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:223\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    221\u001b[0m     model_config)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:25\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/executor_base.py:41\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_language_config \u001b[38;5;241m=\u001b[39m vision_language_config\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:63\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     result_handler\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_monitor\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributed_init_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_init_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m                   max_concurrent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     68\u001b[0m                   max_parallel_loading_workers)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:67\u001b[0m, in \u001b[0;36mGPUExecutor._create_worker\u001b[0;34m(self, local_rank, rank, distributed_init_method)\u001b[0m\n\u001b[1;32m     61\u001b[0m     worker_class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_spec_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m WorkerWrapperBase(\n\u001b[1;32m     64\u001b[0m     worker_module_name\u001b[38;5;241m=\u001b[39mworker_module_name,\n\u001b[1;32m     65\u001b[0m     worker_class_name\u001b[38;5;241m=\u001b[39mworker_class_name,\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_worker_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mdistributed_init_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mworker\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker_base.py:134\u001b[0m, in \u001b[0;36mWorkerWrapperBase.init_worker\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_module_name)\n\u001b[1;32m    133\u001b[0m worker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class_name)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker \u001b[38;5;241m=\u001b[39m \u001b[43mworker_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py:75\u001b[0m, in \u001b[0;36mWorker.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, local_rank, rank, distributed_init_method, lora_config, vision_language_config, speculative_config, is_driver_worker)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config, (\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be tested: vision language model with LoRA settings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m ModelRunnerClass \u001b[38;5;241m=\u001b[39m (EmbeddingModelRunner \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m     74\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode \u001b[38;5;28;01melse\u001b[39;00m ModelRunner)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner \u001b[38;5;241m=\u001b[39m \u001b[43mModelRunnerClass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_driver_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_driver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Uninitialized cache engine. Will be initialized by\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# initialize_cache.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine: CacheEngine\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/model_runner.py:119\u001b[0m, in \u001b[0;36mModelRunner.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, lora_config, kv_cache_dtype, is_driver_worker, vision_language_config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# When using CUDA graph, the input block tables must be padded to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# max_seq_len_to_capture. However, creating the block table in\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Python can be expensive. To optimize this, we cache the block table\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# in numpy and only copy the actual input content at every iteration.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# The shape of the cached block table will be\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# (max batch size to capture, max context len to capture / block size).\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_block_tables \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    117\u001b[0m     (\u001b[38;5;28mmax\u001b[39m(_BATCH_SIZES_TO_CAPTURE), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_block_per_batch()),\n\u001b[1;32m    118\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m \u001b[43mget_attn_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_attention_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_kv_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create processor for multi-modal data\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_language_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/attention/selector.py:43\u001b[0m, in \u001b[0;36mget_attn_backend\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size, is_blocksparse)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocksparse_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m         BlocksparseFlashAttentionBackend)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BlocksparseFlashAttentionBackend\n\u001b[0;32m---> 43\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mwhich_attn_to_use\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         FlashAttentionBackend)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/attention/selector.py:129\u001b[0m, in \u001b[0;36mwhich_attn_to_use\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# FlashAttn in NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# Volta and Turing NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use FlashAttention-2 backend for Volta and Turing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m         selected_backend \u001b[38;5;241m=\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mXFORMERS\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:430\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:312\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             )\n\u001b[0;32m--> 312\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. device=\u0004, num_gpus=\u0004\n\nCUDA call was originally invoked at:\n\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_27318/2183081615.py\", line 2, in <module>\n    vectorstore=get_vectorstore(text_chunks)\n  File \"/tmp/ipykernel_27318/1057278163.py\", line 3, in get_vectorstore\n    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\n    return wrapped(self, *args, **kwargs)\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 71, in __init__\n    import sentence_transformers\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/__init__.py\", line 7, in <module>\n    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py\", line 1, in <module>\n    from .CrossEncoder import CrossEncoder\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\", line 7, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/__init__.py\", line 1478, in <module>\n    _C._initExtension(manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 238, in <module>\n    _lazy_call(_check_capability)\n  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=31361)\u001b[0;0m INFO 08-01 20:50:41 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31361)\u001b[0;0m INFO 08-01 20:50:41 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31362)\u001b[0;0m INFO 08-01 20:50:41 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31362)\u001b[0;0m INFO 08-01 20:50:41 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31360)\u001b[0;0m INFO 08-01 20:50:41 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31360)\u001b[0;0m INFO 08-01 20:50:41 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31361)\u001b[0;0m INFO 08-01 20:50:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31362)\u001b[0;0m INFO 08-01 20:50:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=31360)\u001b[0;0m INFO 08-01 20:50:42 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[36m(RayWorkerWrapper pid=139083)\u001b[0m INFO 08-01 22:29:22 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[36m(RayWorkerWrapper pid=139083)\u001b[0m INFO 08-01 22:29:22 selector.py:51] Using XFormers backend.\n"
     ]
    }
   ],
   "source": [
    "conversation_chain = get_conversationchain(vectorstore)\n",
    "handle_question(conversation_chain, \"where will ARA travel to for CBOA 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s, est. speed input: 339.20 toks/s, output: 54.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 365.96 toks/s, output: 63.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: where will ARA travel to for CBOA 2024?\n",
      "Bot: \n",
      "ARA will travel to Camp Lejeune, North Carolina for the CBOA 2024 event.\n",
      "User: The FFP includes hours to what?\n",
      "Bot: \n",
      "To answer this question, you would need to look at the FFP for the entire project, which includes both phases. The quote states that the total FFP is $14,400.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handle_question(conversation_chain, \"The FFP includes hours to what?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s, est. speed input: 421.16 toks/s, output: 52.14 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 248.73 toks/s, output: 64.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: where will ARA travel to for CBOA 2024?\n",
      "Bot: \n",
      "ARA will travel to Camp Lejeune, North Carolina for the CBOA 2024 event.\n",
      "User: what is the scope of work for video production for FAM-S?\n",
      "Bot:  The scope of video production for FAM-S is to create a quality training video that covers the basic operations of the system, including maintenance/cleaning, and troubleshooting. The video will be approximately 10 minutes in duration and will feature video shots, graphics, and audio elements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52092)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52093)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52094)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52096)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52095)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52097)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 294, in start_worker_execution_loop\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     while self._execute_model_non_driver():\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 303, in _execute_model_non_driver\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     data = broadcast_tensor_dict(src=0)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 399, in broadcast_tensor_dict\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     torch.distributed.broadcast_object_list(recv_metadata_list,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     broadcast(object_sizes_tensor, src=src, group=group)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]   File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226]     work.wait()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52098)\u001b[0;0m ERROR 07-09 19:11:16 multiproc_worker_utils.py:226] \n"
     ]
    }
   ],
   "source": [
    "handle_question(conversation_chain, \"what is the scope of work for video production for FAM-S?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [172.30.3.201]:8087",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhandle_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwat does each module of the FAM-S training video consist of?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mhandle_question\u001b[0;34m(conversation_chain, question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_question\u001b[39m(conversation_chain, question):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconversation_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chat_history):\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    381\u001b[0m }\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:147\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_history_str:\n\u001b[1;32m    146\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m--> 147\u001b[0m     new_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     new_question \u001b[38;5;241m=\u001b[39m question\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:605\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    606\u001b[0m         _output_key\n\u001b[1;32m    607\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m     )\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    381\u001b[0m }\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/llm.py:127\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    124\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain/chains/llm.py:139\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    147\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    148\u001b[0m     )\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/langchain_community/llms/vllm.py:132\u001b[0m, in \u001b[0;36mVLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# call the model\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/utils.py:691\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    687\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m    688\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m    689\u001b[0m         )\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:304\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request)\u001b[0m\n\u001b[1;32m    296\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    299\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    300\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    301\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    302\u001b[0m )\n\u001b[0;32m--> 304\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/entrypoints/llm.py:556\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    554\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 556\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py:776\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    768\u001b[0m     execute_model_req \u001b[38;5;241m=\u001b[39m ExecuteModelRequest(\n\u001b[1;32m    769\u001b[0m         seq_group_metadata_list\u001b[38;5;241m=\u001b[39mseq_group_metadata_list,\n\u001b[1;32m    770\u001b[0m         blocks_to_swap_in\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_in,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         running_queue_size\u001b[38;5;241m=\u001b[39mscheduler_outputs\u001b[38;5;241m.\u001b[39mrunning_queue_size,\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 776\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:76\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_worker_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_worker_execution_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     72\u001b[0m         async_run_remote_workers_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver_execute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:84\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._driver_execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_driver_execute_model\u001b[39m(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     77\u001b[0m     execute_model_req: Optional[ExecuteModelRequest] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[SamplerOutput]:\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run execute_model in the driver worker.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Passing None will cause the driver to stop the model execution\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    loop running in each of the remote workers.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/worker/worker.py:272\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    263\u001b[0m blocks_to_copy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(execute_model_req\u001b[38;5;241m.\u001b[39mblocks_to_copy,\n\u001b[1;32m    264\u001b[0m                               device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    265\u001b[0m                               dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    266\u001b[0m data: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_seq_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_seq_groups,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks_to_swap_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: blocks_to_swap_in,\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks_to_swap_out\u001b[39m\u001b[38;5;124m\"\u001b[39m: blocks_to_swap_out,\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks_to_copy\u001b[39m\u001b[38;5;124m\"\u001b[39m: blocks_to_copy,\n\u001b[1;32m    271\u001b[0m }\n\u001b[0;32m--> 272\u001b[0m \u001b[43mbroadcast_tensor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_swap(blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# If there is no input, we don't need to execute the model.\u001b[39;00m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/communication_op.py:32\u001b[0m, in \u001b[0;36mbroadcast_tensor_dict\u001b[0;34m(tensor_dict, src)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor_dict\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tp_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/vllm/distributed/parallel_state.py:373\u001b[0m, in \u001b[0;36mGroupCoordinator.broadcast_tensor_dict\u001b[0;34m(self, tensor_dict, src, group, metadata_group)\u001b[0m\n\u001b[1;32m    369\u001b[0m metadata_list, tensor_list \u001b[38;5;241m=\u001b[39m _split_tensor_dict(tensor_dict)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# `metadata_list` lives in CPU memory.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# `broadcast_object_list` has serialization & deserialization,\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# all happening on CPU. Therefore, we can use the CPU group.\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_object_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m async_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensor_list:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:75\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     77\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2649\u001b[0m, in \u001b[0;36mbroadcast_object_list\u001b[0;34m(object_list, src, group, device)\u001b[0m\n\u001b[1;32m   2646\u001b[0m     object_sizes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(object_list), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mcurrent_device)\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;66;03m# Broadcast object sizes\u001b[39;00m\n\u001b[0;32m-> 2649\u001b[0m \u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_sizes_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;66;03m# Concatenate and broadcast serialized object tensors\u001b[39;00m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;66;03m# Note: torch.cat will do an extra memory copy to the current device, if the tensor_list\u001b[39;00m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;66;03m# has only one element, we can skip the copy.\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_rank \u001b[38;5;241m==\u001b[39m src:\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:75\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     77\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/agent_testing/.conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2144\u001b[0m, in \u001b[0;36mbroadcast\u001b[0;34m(tensor, src, group, async_op)\u001b[0m\n\u001b[1;32m   2142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m work\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2144\u001b[0m     \u001b[43mwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [172.30.3.201]:8087"
     ]
    }
   ],
   "source": [
    "handle_question(conversation_chain, \"wat does each module of the FAM-S training video consist of?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    print(\"Chat with multiple PDFs\")\n",
    "\n",
    "    pdf_paths = input(\"Enter paths to your PDFs (comma separated): \").split(',')\n",
    "    question = input(\"Ask a question from your document: \")\n",
    "    \n",
    "    if pdf_paths and question:\n",
    "        docs = [path.strip() for path in pdf_paths]\n",
    "        \n",
    "        # Get the PDF text\n",
    "        raw_text = get_pdf_text(docs)\n",
    "        \n",
    "        # Get the text chunks\n",
    "        text_chunks = get_chunks(raw_text)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = get_vectorstore(text_chunks)\n",
    "        \n",
    "        # Create conversation chain\n",
    "        conversation_chain = get_conversationchain(vectorstore)\n",
    "        \n",
    "        # Handle the question\n",
    "        handle_question(conversation_chain, question)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1: Set up the LLM and Vector Store\n",
    "llm = VLLM(\n",
    "    model=\"baichuan-inc/Baichuan2-13B-Chat\",\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=512,\n",
    "    top_k=20,\n",
    "    top_p=0.8,\n",
    "    temperature=0.8,\n",
    "    dtype=\"float16\",\n",
    "    tensor_parallel_size=8\n",
    ")\n",
    "\n",
    "# Define the PromptTemplate for extracting claims\n",
    "claim_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "    Identify the main claims and conclusions from the following text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define a function to extract claims using LLMChain\n",
    "def extract_claims(paragraphs):\n",
    "    chain = LLMChain(llm=llm, prompt=claim_prompt)\n",
    "    claims = [chain.run({\"text\": para}) for para in paragraphs]\n",
    "    return claims\n",
    "\n",
    "# Step 2: Identify claims and conclusions in the text chunks\n",
    "# Example chunks (replace with actual text chunks)\n",
    "introduction_chunks = [\"...\"]  # Add introduction paragraphs here\n",
    "related_work_chunks = [\"...\"]  # Add related work paragraphs here\n",
    "conclusion_chunks = [\"...\"]  # Add conclusion paragraphs here\n",
    "\n",
    "# Extract claims\n",
    "intro_claims = extract_claims(introduction_chunks)\n",
    "related_work_claims = extract_claims(related_work_chunks)\n",
    "conclusion_claims = extract_claims(conclusion_chunks)\n",
    "\n",
    "# Step 3: Summarize and Compile the Literature Survey\n",
    "def summarize_claims(claims, section_name):\n",
    "    summary_template = PromptTemplate(\n",
    "        input_variables=[\"claims\"],\n",
    "        template=\"\"\"\n",
    "        Summarize the following claims from the {section_name} section in a concise manner:\n",
    "        {claims}\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=summary_template)\n",
    "    summary = chain.run({\"claims\": \" \".join(claims), \"section_name\": section_name})\n",
    "    return summary\n",
    "\n",
    "intro_summary = summarize_claims(intro_claims, \"Introduction\")\n",
    "related_work_summary = summarize_claims(related_work_claims, \"Related Work\")\n",
    "conclusion_summary = summarize_claims(conclusion_claims, \"Conclusion\")\n",
    "\n",
    "# Compile the final literature survey report\n",
    "literature_survey = f\"\"\"\n",
    "Literature Survey Report\n",
    "\n",
    "Introduction:\n",
    "{intro_summary}\n",
    "\n",
    "Related Work:\n",
    "{related_work_summary}\n",
    "\n",
    "Conclusion:\n",
    "{conclusion_summary}\n",
    "\"\"\"\n",
    "\n",
    "print(literature_survey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-21 21:44:06 config.py:1222] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-07-21 21:44:09,252\tINFO worker.py:1770 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-21 21:44:10 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 07-21 21:44:10 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='baichuan-inc/Baichuan2-13B-Chat', speculative_config=None, tokenizer='baichuan-inc/Baichuan2-13B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=baichuan-inc/Baichuan2-13B-Chat)\n",
      "WARNING 07-21 21:44:10 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 07-21 21:44:11 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-21 21:44:11 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:13 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:13 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:14 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:14 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:17 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:17 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m WARNING 07-21 21:44:18 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_cbb87664'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:18 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:18 selector.py:51] Using XFormers backend.\n",
      "INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:18 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "INFO 07-21 21:44:45 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:45 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:45 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:45 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:46 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:46 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:46 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:46 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "INFO 07-21 21:44:51 distributed_gpu_executor.py:56] # GPU blocks: 5573, # CPU blocks: 2621\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:44:55 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40140)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40144)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40139)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40141)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40142)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40143)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=40145)\u001b[0;0m INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "INFO 07-21 21:45:11 model_runner.py:965] Graph capturing finished in 16 secs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Step 1: Set up the LLM\n",
    "llm = VLLM(\n",
    "    model=\"baichuan-inc/Baichuan2-13B-Chat\",\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=512,\n",
    "    top_k=20,\n",
    "    top_p=0.8,\n",
    "    temperature=0.8,\n",
    "    dtype=\"float16\",\n",
    "    tensor_parallel_size=8\n",
    ")\n",
    "\n",
    "# Define the PromptTemplate for extracting claims\n",
    "claim_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "    Identify the main claims and conclusions that are the most important from the following text. Do not use any of your own wording just simple extract the main claims and conclusions made or any opinion by the paper:\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the PromptTemplate for summarizing claims\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"claims\"],\n",
    "    template=\"\"\"\n",
    "    Summarize the following claims in a concise manner:\n",
    "    {claims}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Function to extract claims using LLMChain\n",
    "def extract_claims(paragraph):\n",
    "    chain = LLMChain(llm=llm, prompt=claim_prompt)\n",
    "    return chain.run({\"text\": paragraph})\n",
    "\n",
    "# Function to summarize claims using LLMChain\n",
    "def summarize_claims(claims):\n",
    "    chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "    return chain.run({\"claims\": \" \".join(claims)})\n",
    "\n",
    "# Step 1: Extract claims from each paragraph\n",
    "def process_chunks(chunks):\n",
    "    claims = []\n",
    "    for paragraph in chunks:\n",
    "        claims.append(extract_claims(paragraph))\n",
    "    return claims\n",
    "\n",
    "# Example chunks (replace with actual text chunks)\n",
    "introduction_chunks = [\"\"\"Should we need references for the evaluation of image captions? After all, when humans assess the appropriateness of an image caption, we do so just\n",
    "by looking at the image and reading the candidate’s text.1 See Elliott and Keller (2014) and Kilickaya et al. (2017)for thorough comparisons of caption generation metrics.\n",
    "arXiv:2104.08718v3 [cs.CV] 23 Mar 2022A recent trend in machine translation serves asinspiration: there, a key hurdle for reference-free\n",
    "evaluation (sometimes called quality estimation) has been estimating cross-lingual similarity between source+candidate pairs (Blatz et al., 2004;\n",
    "Specia et al., 2010; Mehdad et al., 2012; Specia and Shah, 2018). But recent work (Lo, 2019; Yankovskaya et al., 2019; Zhao et al., 2020) has\n",
    "improved correlation with human judgment not by gathering more monolingual references, but instead by utilizing cross-lingual representations learned\n",
    "by large-scale, pre-trained, multilingual models e.g., LASER (Artetxe and Schwenk, 2019) or MBERT (Devlin et al., 2019).\"\"\", \"\"\"We hypothesize that the relationships learned by\n",
    "pretrained vision+language models (e.g., ALIGN (Jia et al., 2021) and CLIP (Radford et al., 2021)) could similarly support reference-free evaluation\n",
    "in the image captioning case. Indeed, they can: we show that a relatively direct application of CLIP to (image, generated caption) pairs results in surprisingly high correlation with human judgments\n",
    "on a suite of standard image description benchmarks (e.g., MSCOCO (Lin et al., 2014)). We call this process CLIPScore (abbreviated to CLIP-S).\n",
    "Beyond direct correlation with human judgments, an information gain analysis reveals that CLIP-S is complementary both to commonly reported metrics\n",
    "(like BLEU-4, SPICE, and CIDEr) and to newly proposed reference-base\"\"\"]  # Replace with actual paragraphs\n",
    "related_work_chunks = [\"\"\"Reference-only image caption evaluation In general, image caption generation models are evaluated by a suite of 5 reference based metrics:\n",
    "BLEU-4 (Papineni et al., 2002) (which measures a version of precision between a candidate and the references), ROUGE-L (Lin, 2004) (which measures a version of recall), METEOR (Banerjee and\n",
    "Lavie, 2005) (which computes a word-level alignment), CIDEr (Vedantam et al., 2015) (which combines n-gram tf-idf weighting and stemming) and\n",
    "SPICE (Anderson et al., 2016) (which applies a semantic parser to a set of references, and computes similarity using the predicted scene graph).3\n",
    "Yi et al. (2020) give a method for re-weighting BERTScore (Zhang et al., 2020) specifically tuned to the image caption generation domain (we refer\n",
    "to their method as BERT-S++)\"\"\"]  # Replace with actual paragraphs\n",
    "conclusion_chunks = [\"\"\"For literal image description tasks, CLIPScore\n",
    "achieves high correlation with human judgments\n",
    "of caption quality without references when used in\n",
    "an off-the-shelf fashion. Additional experiments\n",
    "in divergent domains suggest that CLIP can also\n",
    "reason about non-photographic clip-art, and serves\n",
    "as a reasonable option for reference-free evaluation\n",
    "in the alt-text case. Promising future work includes\n",
    "exploring 1) CLIP-S as a reinforcement learning reward for literal caption generators; and 2) whether\n",
    "a small amount of labelled human rating data could\n",
    "help CLIP-S adapt to domains where it struggles,\n",
    "e.g., engagingness prediction. We hope our work\n",
    "can contribute to the ongoing discussion about the\n",
    "role of pretrained models in generation evaluation.\n",
    "\"\"\", \"\"\"Reference-free evaluation runs some risks.\n",
    "Much like BERTScore, model-based metrics like\n",
    "CLIP-S reflect the biases of the pre-training data.\n",
    "While we believe that using CLIP-S as an offline\n",
    "evaluation metric for literal caption quality accords\n",
    "with the recommendations of CLIP’s model card18\n",
    "(Mitchell et al., 2019), Agarwal et al. (2021)’s\n",
    "study demonstrates that CLIP can make disproportionate incorrect classifications of people, e.g.,\n",
    "“male images were misclassified into classes related to crime.” Exploring potential social biases of\n",
    "candidate generations (as in, e.g., Hendricks et al.\n",
    "(2018)) remains paramount, particularly if a system\n",
    "is to be deployed.\"\"\"]  # Replace with actual paragraphs\n",
    "\n",
    "intro_claims = process_chunks(introduction_chunks)\n",
    "related_work_claims = process_chunks(related_work_chunks)\n",
    "conclusion_claims = process_chunks(conclusion_chunks)\n",
    "\n",
    "# Step 2: Summarize each set of claims\n",
    "def summarize_chunks(claims):\n",
    "    summaries = []\n",
    "    for claim in claims:\n",
    "        summaries.append(summarize_claims(claim))\n",
    "    return summaries\n",
    "\n",
    "intro_summaries = summarize_chunks(intro_claims)\n",
    "related_work_summaries = summarize_chunks(related_work_claims)\n",
    "conclusion_summaries = summarize_chunks(conclusion_claims)\n",
    "\n",
    "# Step 3: Concatenate summaries to form the final literature survey\n",
    "final_intro_summary = \" \".join(intro_summaries)\n",
    "final_related_work_summary = \" \".join(related_work_summaries)\n",
    "final_conclusion_summary = \" \".join(conclusion_summaries)\n",
    "\n",
    "# Compile the final literature survey report\n",
    "literature_survey = f\"\"\"\n",
    "Literature Survey Report\n",
    "\n",
    "Introduction:\n",
    "{final_intro_summary}\n",
    "\n",
    "Related Work:\n",
    "{final_related_work_summary}\n",
    "\n",
    "Conclusion:\n",
    "{final_conclusion_summary}\n",
    "\"\"\"\n",
    "\n",
    "print(literature_survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 97.64 toks/s, output: 69.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it, est. speed input: 59.71 toks/s, output: 70.85 toks/s]\n"
     ]
    }
   ],
   "source": [
    "conclusion_claims = process_chunks(conclusion_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Main Claims and Conclusions:\\n    1. CLIPScore achieves high correlation with human judgments of caption quality without references when used in an off-the-shelf fashion for literal image description tasks.\\n    2. CLIP can also reason about non-photographic clip-art, making it a reasonable option for reference-free evaluation in the alt-text case.\\n    3. Promising future work includes exploring the use of CLIP-S as a reinforcement learning reward for literal caption generators and whether a small amount of labelled human rating data could help CLIP-S adapt to domains where it struggles.\\n    4. The authors hope their work can contribute to the ongoing discussion about the role of pretrained models in generation evaluation.',\n",
       " 'We encourage researchers to continue exploring\\nand comparing metrics for this purpose.\\n    The authors acknowledge that CLIP-S may also be\\nbiased, and we call for further research into the biases\\nof CLIP-S and other metrics.\\n\\n    In conclusion, CLIP-S can be a useful metric for\\nevaluating literal caption quality, but it is not a perfect\\nindicator of overall image-text alignment. We encourage\\nresearchers to continue exploring and comparing metrics\\nfor this purpose.\\n\\nThe main claims and conclusions from the text are:\\n1. CLIP-S can be used as an offline evaluation metric for literal caption quality, as per the recommendations of CLIP’s model card.\\n2. However, CLIP can make disproportionate incorrect classifications of people, e.g., male images were misclassified into classes related to crime.\\n3. Exploring potential social biases of candidate generations is paramount, particularly if a system is to be deployed.\\n4. CLIP-S may also be biased, and further research into the biases of CLIP-S and other metrics is required.\\n5. CLIP-S can be a useful metric for evaluating literal caption quality, but it is not a perfect indicator of overall image-text alignment.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_chunks = [\"\"\"For literal image description tasks, CLIPScore\n",
    "achieves high correlation with human judgments\n",
    "of caption quality without references when used in\n",
    "an off-the-shelf fashion. Additional experiments\n",
    "in divergent domains suggest that CLIP can also\n",
    "reason about non-photographic clip-art, and serves\n",
    "as a reasonable option for reference-free evaluation\n",
    "in the alt-text case. Promising future work includes\n",
    "exploring 1) CLIP-S as a reinforcement learning reward for literal caption generators; and 2) whether\n",
    "a small amount of labelled human rating data could\n",
    "help CLIP-S adapt to domains where it struggles,\n",
    "e.g., engagingness prediction. We hope our work\n",
    "can contribute to the ongoing discussion about the\n",
    "role of pretrained models in generation evaluation.\n",
    "\"\"\", \"\"\"Reference-free evaluation runs some risks.\n",
    "Much like BERTScore, model-based metrics like\n",
    "CLIP-S reflect the biases of the pre-training data.\n",
    "While we believe that using CLIP-S as an offline\n",
    "evaluation metric for literal caption quality accords\n",
    "with the recommendations of CLIP’s model card18\n",
    "(Mitchell et al., 2019), Agarwal et al. (2021)’s\n",
    "study demonstrates that CLIP can make disproportionate incorrect classifications of people, e.g.,\n",
    "“male images were misclassified into classes related to crime.” Exploring potential social biases of\n",
    "candidate generations (as in, e.g., Hendricks et al.\n",
    "(2018)) remains paramount, particularly if a system\n",
    "is to be deployed.\"\"\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.29s/it, est. speed input: 106.41 toks/s, output: 70.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it, est. speed input: 888.20 toks/s, output: 64.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Summarize the following claims in a consise and small manner. do not add anything that might not be relevant for summarizing this into a literature survey:\n",
      "    M a i n   C l a i m s   a n d   C o n c l u s i o n s : \n",
      "         1 .   C L I P S c o r e   a c h i e v e s   h i g h   c o r r e l a t i o n   w i t h   h u m a n   j u d g m e n t s   o f   c a p t i o n   q u a l i t y   w i t h o u t   r e f e r e n c e s   w h e n   u s e d   i n   a n   o f f - t h e - s h e l f   f a s h i o f i o n   f o r   l i t e r a l   i m a g e   d e s c r i p t i o n   t a s k s . \n",
      "         2 .   C L I P   c a n   a l s o   r e a s o n a b o u t   n o n - p h o t o g r a p h i c   c l i p - a r t ,   m a k i n g   i t   a   r e a s o n a b l e   o p t i o n   f o r   r e f e r e n c e - f r e e   e v a l u a t i o n   i n   t h e   a l t - t e x t   c a s e . \n",
      "         3 .   P r o m i s i n g   f u t u r e   w o r k   i n c l u d e s   e x p l o r i n g   t h e   u s e   o f   C L I P - S   a s   a   r e i n f o r c e m e n t   l e a r n i n g   r e w a r d   f o r   l i t e r a l   c a p t i o \n",
      "    W e   e n c o u r a g e   r e s e a r c h e r s   t o   c o n t i n u e   e x p l o r i n g   a n d   c o m p a r i n g   m e t r i c s \n",
      " f o r   t h i s   p u r p o s e .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Summarize each set of claims\n",
    "def summarize_chunks(claims):\n",
    "    summaries = []\n",
    "    for claim in claims:\n",
    "        summaries.append(summarize_claims(claim))\n",
    "    return summaries\n",
    "\n",
    "\n",
    "conclusion_summaries = summarize_chunks(conclusion_claims)\n",
    "\n",
    "\n",
    "final_conclusion_summary = \" \".join(conclusion_summaries)\n",
    "\n",
    "print(final_conclusion_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"claims\"],\n",
    "    template=\"\"\"\n",
    "    Summarize the following claims in a consise and small manner. do not add anything that might not be relevant for summarizing this into a literature survey:\n",
    "    {claims}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Function to summarize claims using LLMChain\n",
    "def summarize_claims(claims):\n",
    "    chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "    return chain.run({\"claims\": \" \".join(claims)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 415 PDF files in the folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_pdfs_in_folder(folder_path):\n",
    "    # List all files in the specified folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Count the number of PDF files\n",
    "    pdf_count = sum(1 for file in files if file.lower().endswith('.pdf'))\n",
    "    return pdf_count\n",
    "\n",
    "# Specify the path to your folder\n",
    "folder_path = 'pdf_proposals'\n",
    "\n",
    "# Get the count of PDF files\n",
    "pdf_count = count_pdfs_in_folder(folder_path)\n",
    "print(f'There are {pdf_count} PDF files in the folder.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-01 22:46:52 config.py:1222] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-01 22:46:54,798\tINFO worker.py:1770 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 22:46:55 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 08-01 22:46:55 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='baichuan-inc/Baichuan2-13B-Chat', speculative_config=None, tokenizer='baichuan-inc/Baichuan2-13B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=baichuan-inc/Baichuan2-13B-Chat)\n",
      "WARNING 08-01 22:46:56 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 08-01 22:46:57 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-01 22:46:57 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "INFO 08-01 22:46:59 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:46:59 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:01 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:02 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m WARNING 08-01 22:47:03 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_e26434bd'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:04 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:27 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "INFO 08-01 22:47:29 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:29 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:30 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:30 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:30 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:30 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:30 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "INFO 08-01 22:47:35 distributed_gpu_executor.py:56] # GPU blocks: 5573, # CPU blocks: 2621\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:40 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318965)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318970)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318968)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318966)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318971)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 15 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318969)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=318967)\u001b[0;0m INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n",
      "INFO 08-01 22:47:55 model_runner.py:965] Graph capturing finished in 16 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 25.35 toks/s, output: 70.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cake is an essential part of any bakery because it is a delicious and popular dessert. It not only satisfies customers' sweet tooth but also adds variety to the bakery's menu. Cake can be customized to different flavors, textures, and decorations, allowing bakeries to cater to various preferences and occasions. Additionally, cake is a great way to showcase the baker's creativity and skill, making it a significant aspect of any bakery's success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def get_conversation_chain():\n",
    "    llm = VLLM(\n",
    "        model=\"baichuan-inc/Baichuan2-13B-Chat\",\n",
    "        trust_remote_code=True,  # mandatory for hf models\n",
    "        max_new_tokens=4096,\n",
    "        top_k=20,\n",
    "        top_p=0.8,\n",
    "        temperature=0.8,\n",
    "        dtype=\"float16\",\n",
    "        tensor_parallel_size=8\n",
    "    )\n",
    "    \n",
    "    # Define the prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"\n",
    "        Given the question, generate a conversational response.\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Response:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create the LLMChain\n",
    "    conversation_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt_template\n",
    "    )\n",
    "    \n",
    "    return conversation_chain\n",
    "\n",
    "# Example usage\n",
    "# Initialize the conversation chain\n",
    "chain = get_conversation_chain()\n",
    "\n",
    "# Ask a simple question\n",
    "question = \"What is the significance of cake in bakery?\"\n",
    "response = chain.run(question=question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s, est. speed input: 1281.96 toks/s, output: 36.62 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        'no'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"Automatic evaluation of natural language generation, for example in machine translation and caption\\ngeneration, requires comparing candidate sentences to annotated references. The goal is to evaluate\\nsemantic equivalence. However, commonly used methods rely on surface-form similarity only. For\\nexample, BLEU (Papineni et al., 2002), the most common machine translation metric, simply counts\\nn-gram overlap between the candidate and the reference. While this provides a simple and general\\nmeasure, it fails to account for meaning-preserving lexical and compositional diversity.\"\n",
    "paragraph = \"Future research is needed to explore this further.\"\n",
    "template=\"Evaluate the following paragraph to determine if it contains a conclusion, claim, or opinion that is substantiated and can contribute to a literature review discussing the pros and cons of this method. NOTE - Exclude any sections that are purely methodological or lack substantiation. Be extremely strict with these requirements. Use the examples provided as a guide. Answer with 'yes' only if confident in its usefulness, otherwise 'no'. Only respond with 'yes' or 'no':\\n\\n\"+paragraph+\"\\n\\nExamples of valid content: \\n1. 'This method significantly improves accuracy compared to previous approaches.'\\n2. 'The technique offers a novel perspective that challenges conventional theories.'\\n\\nExamples of invalid content:\\n1. 'The methodology involved multiple regression analyses.'\\n2. 'Future research is needed to explore this further.'\\n\\nAnswer:\"\n",
    "response = chain.run(question=template)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
