{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n",
    "\n",
    "\n",
    "def get_pdf_text(docs):\n",
    "    text=\"\"\n",
    "    for pdf in docs:\n",
    "        pdf_reader=PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text+=page.extract_text()\n",
    "    return text\n",
    "\n",
    "# converting text to chunks\n",
    "def get_chunks(raw_text):\n",
    "    text_splitter=CharacterTextSplitter(separator=\"\\n\",\n",
    "                                        chunk_size=1000,\n",
    "                                        chunk_overlap=200,\n",
    "                                        length_function=len)   \n",
    "    chunks=text_splitter.split_text(raw_text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectorstore(chunks):\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                     model_kwargs={'device':'cpu'})\n",
    "    vectorstore=faiss.FAISS.from_texts(texts=chunks,embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# generating conversation chain  \n",
    "\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def get_conversationchain(vectorstore):\n",
    "    llm = VLLM(\n",
    "        model=\"baichuan-inc/Baichuan2-13B-Chat\",\n",
    "        trust_remote_code=True,  # mandatory for hf models\n",
    "        max_new_tokens=512,\n",
    "        top_k=20,\n",
    "        top_p=0.8,\n",
    "        temperature=0.8,\n",
    "        dtype=\"float16\",\n",
    "        tensor_parallel_size=8\n",
    "        )\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', \n",
    "                                      return_messages=True,\n",
    "                                      output_key='answer') \n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                                llm=llm,\n",
    "                                retriever=vectorstore.as_retriever(),\n",
    "                                condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "                                memory=memory)\n",
    "    return conversation_chain\n",
    "\n",
    "\n",
    "def handle_question(conversation_chain, question):\n",
    "    response = conversation_chain({'question': question})\n",
    "    chat_history = response[\"chat_history\"]\n",
    "    for i, msg in enumerate(chat_history):\n",
    "        if i % 2 == 0:\n",
    "            print(f\"User: {msg.content}\")\n",
    "        else:\n",
    "            print(f\"Bot: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-01 20:23:58 config.py:1222] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-07-01 20:24:00,884\tINFO worker.py:1770 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 20:24:02 config.py:623] Defaulting to use mp for distributed inference\n",
      "INFO 07-01 20:24:02 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='baichuan-inc/Baichuan2-13B-Chat', speculative_config=None, tokenizer='baichuan-inc/Baichuan2-13B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=baichuan-inc/Baichuan2-13B-Chat)\n",
      "WARNING 07-01 20:24:02 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 07-01 20:24:03 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-01 20:24:03 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:05 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:05 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:05 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:05 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:05 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:05 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:05 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:05 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:06 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:06 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:06 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:06 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:06 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:06 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:08 utils.py:637] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:08 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m WARNING 07-01 20:24:09 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/agent_testing/.conda/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\n",
      "    cache[rtype].remove(name)\n",
      "KeyError: '/psm_7ed30a86'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:09 selector.py:51] Using XFormers backend.\n",
      "INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:10 weight_utils.py:218] Using model weights format ['*.bin']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:34 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:35 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "INFO 07-01 20:24:35 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:35 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:36 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:36 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:36 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:36 model_runner.py:160] Loading model weights took 3.2593 GB\n",
      "INFO 07-01 20:24:41 distributed_gpu_executor.py:56] # GPU blocks: 5573, # CPU blocks: 2621\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:24:45 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566251)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566249)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566245)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566248)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566246)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566250)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=566247)\u001b[0;0m INFO 07-01 20:25:05 model_runner.py:965] Graph capturing finished in 20 secs.\n",
      "INFO 07-01 20:25:06 model_runner.py:965] Graph capturing finished in 20 secs.\n"
     ]
    }
   ],
   "source": [
    "docs = [\"metareview2023.pdf\"]\n",
    "raw_text=get_pdf_text(docs)\n",
    "\n",
    "text_chunks=get_chunks(raw_text)\n",
    "vectorstore=get_vectorstore(text_chunks)\n",
    "\n",
    "conversation_chain = get_conversationchain(vectorstore)\n",
    "#handle_question(conversation_chain, \"what standard metrics are used to access relevance, factual consistency and semantic coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s, est. speed input: 1690.56 toks/s, output: 49.49 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 1685.68 toks/s, output: 58.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what standard metrics are used to access relevance, factual consistency and semantic coherence\n",
      "Bot:  The standard metrics used to access relevance, factual consistency, and semantic coherence are ROUGE-L (Lin, 2004), which quantifies the similarity between the generated and reference texts by calculating the Longest Common Subsequence, and NLI (Natural Language Inference) models for inconsistency detection.\n",
      "User: what are the results\n",
      "Bot:  Relevance is assessed using ROUGE-L (Lin, 2004), NLI models for inconsistency detection, and DiscoScore (Zhao et al., 2022) for coherence indicator.\n",
      "User: what datasets are being used for comparison\n",
      "Bot:  ROUGE-L (Lin, 2004) is used to quantify the similarity between the generated and reference texts by calculating the Longest Common Subsequence. NLI (Natural Language Inference) models are used for inconsistency detection. DiscoScore (Zhao et al., 2022) presents six BERT-based model variants to measure discourse coherence. The scores from these six models are averaged as the coherence indicator.\n",
      "User: what datasets are being used for comparison\n",
      "Bot:  BERTScore, ROUGE-L, and SummaC are used to assess relevance, factual consistency, and semantic coherence.\n",
      "User: what datasets are being used for comparison\n",
      "Bot:  BERTScore (Zhang et al., 2020), ROUGE-L (Lin, 2004), and SummaC (Laban et al., 2021) are used to assess relevance, factual consistency, and semantic coherence.\n",
      "User: okay i do not want to know about standard metrics anymore, i want to know about the summarization technique which are abstractive\n",
      "Bot:  OpinionDi-gest (Suhara et al., 2020) extracts opinions from input reviews and trains a seq2seq model that generates a summary from a set of these opinions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handle_question(conversation_chain, \"okay i do not want to know about standard metrics anymore, i want to know about the summarization technique which are abstractive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/agent_testing/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it, est. speed input: 205.62 toks/s, output: 69.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: i want to know about the summarization technique which are abstractive\n",
      "Bot:  I will provide an overview of the main summarization techniques used in NLP, including extractive and abstractive methods. However, the question asks specifically about abstractive techniques, so I will focus on those.\n",
      "\n",
      "1. Extractive methods: These techniques select the most important sentences or phrases from the input text to create the summary. Examples include TF-IDF weighting (Manning and Schüler, 1999), TextRank (Mihalcea and Tarau, 2004), and LexRank (Erkan and Radev, 2004).\n",
      "\n",
      "2. Abstractive methods: These techniques generate a new summary by creating new sentences from the input text. These methods typically involve a two-step process: first, a document is segmented into its constituent opinions or themes; then, a summary is generated from these opinions or themes. Examples include OpinionDigger (Suhara et al., 2020), 3Sent (Goyal et al., 2022), and TCG (Bhaskar et al., 2022).\n",
      "\n",
      "3. Prompting methods: These techniques use a specific prompt to guide the generation of the summary. Examples include 3Sent (Goyal et al., 2022), TCG (Bhaskar et al., 2022), and 3Sent (Goyal et al., 2022).\n",
      "\n",
      "4. Neural methods: These techniques use a neural network to generate the summary. Examples include Seq2Seq models (Sutskever et al., 2014), Transformer models (Vaswani et al., 2017), and GPT-3 (Brown et al., 2020).\n",
      "\n",
      "In conclusion, the main techniques used in NLP for summarization include extractive methods, abstractive methods, prompting methods, and neural methods. Abstractive techniques generate new sentences from the input text, which can provide a more natural and accurate summary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "handle_question(conversation_chain, \"i want to know about the summarization technique which are abstractive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
